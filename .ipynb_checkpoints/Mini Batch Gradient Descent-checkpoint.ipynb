{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp\n",
    "import copy\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward helper methods\n",
    "def sigmoid(value):\n",
    "    return 1.0/(1+exp(value * (-1)))\n",
    "\n",
    "def sigma(matrix_weight, matrix_input, bias=0):\n",
    "    # Prereq: len(arr_weight) = len(arr_input)\n",
    "    matrix_weight = np.asarray(matrix_weight, dtype=np.float)\n",
    "    matrix_input = np.asarray(matrix_input, dtype=np.float)\n",
    "    print(\"Hasil dot matriks : \" + str(matrix_weight.dot(matrix_input.transpose()) + bias))\n",
    "    hasil_sigma = matrix_weight.dot(matrix_input.transpose()) + bias\n",
    "    return (hasil_sigma[0])\n",
    "\n",
    "\n",
    "# hidden_layer = int (number of hidden layers)\n",
    "# nb_nodes = arr[int] (number of nodes per hidden layer)\n",
    "# len_input_matrix = int (number of features)\n",
    "# Output: List of Matrixes\n",
    "# Method: He initialization\n",
    "# Link: https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78\n",
    "def initialize_weights(hidden_layer, nb_nodes, len_input_matrix):\n",
    "    # nb_nodes = nb_nodes.astype(int) #dicek nanti perlu atau ga\n",
    "    arr_weight_this_batch = list()\n",
    "    for i in range(hidden_layer):\n",
    "        if i==0:\n",
    "            nb_nodes_prev = len_input_matrix\n",
    "        else:\n",
    "            nb_nodes_prev = nb_nodes[i-1]\n",
    "        weight_matrix = np.random.randn(nb_nodes[i], nb_nodes_prev) * np.sqrt(2/(nb_nodes_prev+nb_nodes[i]))\n",
    "        arr_weight_this_batch.append(weight_matrix)\n",
    "    \n",
    "    return arr_weight_this_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation and Update Weight helper methods\n",
    "def error(feed_forward_output, target_output):\n",
    "    return 0.5*((target_output-feed_forward_output)**2)\n",
    "\n",
    "def propagate_error_output_layer(feed_forward_output, target_output):\n",
    "    return feed_forward_output*(1-feed_forward_output)*(target_output-feed_forward_output)\n",
    "\n",
    "def propagate_error_hidden_layer_neuron(arr_weight_input, arr_neuron_input, arr_weight_output, arr_neuron_output):\n",
    "    #Input here means input in a single neuron, while output means output of a single neuron\n",
    "    #len(arr_weight) = len(arr_input)\n",
    "    sigma_input, sigma_output = sigma(arr_weight_input, arr_neuron_input), sigma(arr_weight_output, arr_neuron_output)\n",
    "    return sigmoid(sigma_input) * (1 - sigmoid(sigma_input)) * sigma_output\n",
    "\n",
    "# error = neuron's error\n",
    "def update_weight_neuron(weight_prev_prev, weight_prev, learning_rate, momentum, error, input_neuron):\n",
    "    # weight_prev_prev = previous of weight_prev\n",
    "    return weight_prev + weight_prev_prev * momentum + learning_rate*error*input_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_matrix = matrix[float] (data) (asumsi, kolom terakhir adalah hasil klasifikasi)\n",
    "# hidden_layers = int (number of hidden layers)\n",
    "# nb_nodes = arr[int] (number of nodes per hidden layer)\n",
    "# nu = float (momentum)\n",
    "# alfa = float (learning rate)\n",
    "# epoch = int (number of training loops)\n",
    "# batch_size = int (mini-batch)\n",
    "def mini_batch_gradient_descent(input_matrix, hidden_layer, nb_nodes, nu, alfa, epoch, batch_size=1):\n",
    "    \n",
    "    #transpose-slicing, memisah input dan label\n",
    "    col_width = input_matrix.shape[1]\n",
    "    input_data = (input_matrix.transpose()[0:col_width-1]).transpose()\n",
    "    label_data = (input_matrix.transpose()[col_width-1:col_width]).transpose()\n",
    "    #print(input_data, \"\\n\")\n",
    "    #print(label_data, \"\\n\")\n",
    "    \n",
    "    hidden_layer += 1\n",
    "    nb_nodes.append(1) #as total node in output layer, to simplify \n",
    "    for no_epoch in range(epoch):\n",
    "        if no_epoch == 0:\n",
    "            arr_weight_this_batch = initialize_weights(hidden_layer, nb_nodes, col_width-1)\n",
    "        else:\n",
    "            arr_weight_prev_batch = copy.deepcopy(arr_weight_this_batch) # tracking previous state of weights\n",
    "            \n",
    "        for no_input_data in range(len(input_data)):\n",
    "            #Feed Forward\n",
    "            all_sigma_values = list()\n",
    "            for no_hidden_layer in range(hidden_layer):\n",
    "                if no_hidden_layer == 0:\n",
    "                    all_sigma_values.append(sigma(arr_weight_this_batch[no_hidden_layer], input_data[no_input_data]))\n",
    "                else:\n",
    "                    all_sigma_values.append(sigma(arr_weight_this_batch[no_hidden_layer], all_sigma_values[no_hidden_layer-1]))\n",
    "                for no_rows in range(len(all_sigma_values[no_hidden_layer])):\n",
    "                    all_sigma_values[no_hidden_layer][no_rows] = sigmoid(all_sigma_values[no_hidden_layer][no_rows])\n",
    "            #Result of sigma will be array with 1 element only, so it's safe to select like this\n",
    "            error_value = error(all_sigma_values[no_hidden_layer][0], label_data[no_input_data])[0]\n",
    "            print(\"From data in epoch \" + str(no_epoch) + \" and no_input \" + str(no_input_data) + \" has error \" + str(error_value))\n",
    "            print (hidden_layer)\n",
    "            \n",
    "            #Back Propagation\n",
    "            delta = list()\n",
    "            for no_sigma_values in reversed(range(len(all_sigma_values))):\n",
    "                layer = all_sigma_values[no_sigma_values]\n",
    "                if no_sigma_values == len(all_sigma_values) - 1 :\n",
    "                    delta.append(propagate_error_output_layer(layer[0],label_data[no_input_data])[0])\n",
    "                elif no_sigma_values != 0 and no_epoch != 0 :\n",
    "                    delta.append(propagate_error_hidden_layer_neuron(arr_weight_this_batch,layer,arr_weight_prev_batch,all_sigma_values[no_sigma_values - 1])[0])\n",
    "                    \n",
    "            print(delta[0])\n",
    "            #output_error = propagate_error_output_layer(all_sigma_values[no_hidden_layer][0], label_data[no_input_data])\n",
    "            #for no_hidden_layer in range(hidden_layer-1, -1, -1):\n",
    "                #print(no_hidden_layer)\n",
    "            #print(hidden_layer)\n",
    "            #update_weight_neuron(weight_prev_prev, weight_prev, nu, alfa, error, output_neuron):\n",
    "    #return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil dot matriks : [-0.73622617 -0.13571893]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-da9936c8bcee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#print(data_matrix, \"\\n\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmini_batch_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-82-3ea6499c3e82>\u001b[0m in \u001b[0;36mmini_batch_gradient_descent\u001b[1;34m(input_matrix, hidden_layer, nb_nodes, nu, alfa, epoch, batch_size)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                     \u001b[0mall_sigma_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_weight_this_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_hidden_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_sigma_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_hidden_layer\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mno_rows\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_sigma_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_hidden_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                     \u001b[0mall_sigma_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_hidden_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_rows\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_sigma_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_hidden_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_rows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m#Result of sigma will be array with 1 element only, so it's safe to select like this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "# dataset load and preprocess\n",
    "df = pd.read_csv(\"weather.csv\")\n",
    "#print(df.head, \"\\n\")\n",
    "\n",
    "# transform non-numeric data to numeric data (boolean type is already 0 and 1)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "df[\"outlook\"] = encoder.fit_transform(df[\"outlook\"])\n",
    "df[\"play\"] = encoder.fit_transform(df[\"play\"])\n",
    "#print(df.head, \"\\n\")\n",
    "\n",
    "#scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() #feature_range=(0,1) (min_val, max_val)\n",
    "data_matrix = scaler.fit_transform(df) # di tahap ini, data sudah menjadi numpy array\n",
    "#print(data_matrix, \"\\n\")\n",
    "\n",
    "result = mini_batch_gradient_descent(data_matrix, 3, [2,3,4], 0.1 ,0.4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
