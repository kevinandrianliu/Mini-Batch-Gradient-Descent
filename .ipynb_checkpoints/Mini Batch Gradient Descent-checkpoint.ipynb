{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sure working functions\n",
    "def sigmoid(value):\n",
    "    return 1.0/(1+exp(value * (-1)))\n",
    "\n",
    "# output = float\n",
    "def sigma(arr_weight, arr_input, bias=0):\n",
    "    #len(arr_weight) = len(arr_input)\n",
    "    return arr_weight.dot(arr_input.transpose())[0] + bias\n",
    "\n",
    "# hidden_layer = int (number of hidden layers)\n",
    "# nb_nodes = arr[int] (number of nodes per hidden layer)\n",
    "# len_input_matrix = int (number of features)\n",
    "# Output: List of Matrixes\n",
    "# Method: He initialization\n",
    "# Link: https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78\n",
    "def initialize_weights(hidden_layer, nb_nodes, len_input_matrix):\n",
    "    nb_nodes = nb_nodes.astype(int)\n",
    "    arr_weights = list()\n",
    "    for i in range(hidden_layer):\n",
    "        if i==0:\n",
    "            nb_nodes_prev = len_input_matrix\n",
    "        else:\n",
    "            nb_nodes_prev = nb_nodes[i-1]\n",
    "        weight_matrix = np.random.randn(nb_nodes[i], nb_nodes_prev) * np.sqrt(2/(nb_nodes_prev+nb_nodes[i]))\n",
    "        arr_weights.append(weight_matrix)\n",
    "    return arr_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions in progress\n",
    "def error(feed_forward_output, target_output):\n",
    "    return 0.5*((target_output-feed_forward_output)**2)\n",
    "\n",
    "def propagate_error_output_layer(feed_forward_output, target_output):\n",
    "    return feed_forward_output*(1-feed_forward_output)*(target_output-feed_forward_output)\n",
    "\n",
    "def propagate_error_hidden_layer_neuron(arr_weight_input, arr_neuron_input, arr_weight_output, arr_neuron_output):\n",
    "    #Input here means input in a single neuron, while output means output of a single neuron\n",
    "    #len(arr_weight) = len(arr_input)\n",
    "    sigma_input, sigma_output = sigma(arr_weight_input, arr_neuron_input), sigma(arr_weight_output, arr_neuron_output)\n",
    "    return sigmoid(sigma_input) * (1 - sigmoid(sigma_input)) * sigma_output\n",
    "\n",
    "def update_weight_neuron(weight_prev_prev, weight_prev, nu, alfa, error, output_neuron):\n",
    "    # weight_prev_prev = previous of weight_prev\n",
    "    return weight_prev + weight_prev_prev * alfa + nu*error*output_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_matrix = matrix[float] (data) (asumsi, kolom terakhir adalah hasil klasifikasi)\n",
    "# hidden_layers = int (number of hidden layers)\n",
    "# nb_nodes = arr[int] (number of nodes per hidden layer)\n",
    "# nu = float (momentum)\n",
    "# alfa = float (learning rate)\n",
    "# epoch = int (number of training loops)\n",
    "# batch_size = int (mini-batch)\n",
    "# output = FFNN prediction model (list of matrix)\n",
    "def mini_batch_gradient_descent(input_matrix, hidden_layer, nb_nodes, nu, alfa, epoch, batch_size=1):\n",
    "    \n",
    "    #transpose-slicing, memisah input dan label\n",
    "    col_width = input_matrix.shape[1]\n",
    "    input_data = (input_matrix.transpose()[0:col_width-1]).transpose()\n",
    "    label_data = (input_matrix.transpose()[col_width-1:col_width]).transpose()\n",
    "    #print(input_data, \"\\n\")\n",
    "    #print(label_data, \"\\n\")\n",
    "    \n",
    "    # add output layer, dimana node terakhir adalah 1\n",
    "    hidden_layer = hidden_layer + 1\n",
    "    nb_nodes = np.append(nb_nodes, [1])\n",
    "    \n",
    "    FFNN = initialize_weights(hidden_layer, nb_nodes, col_width-1)\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        FFNN_prev = FFNN\n",
    "    \n",
    "    output = 0 #dummy\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input .csv filename: weather.csv\n",
      "File loaded successfuly.\n"
     ]
    }
   ],
   "source": [
    "# dataset load\n",
    "#df = pd.read_csv(\"weather.csv\")\n",
    "csv_string = input(\"Input .csv filename: \")\n",
    "try:\n",
    "    df = pd.read_csv(csv_string)\n",
    "except:\n",
    "    print(\"File not found.\")\n",
    "    ###quit() (di file Python, pake ini)\n",
    "print(\"File loaded successfuly.\")\n",
    "#print(df.head, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset preprocess\n",
    "\n",
    "# transform non-numeric data to numeric data\n",
    "def transform_to_numeric(matrix_data):\n",
    "    types = matrix_data.dtypes\n",
    "    labels = matrix_data.columns.values # because pandas select columns using column names\n",
    "    for i in range(matrix_data.shape[1]):\n",
    "        type_i = types[i]\n",
    "        if (type_i == object):\n",
    "            values = matrix_data[labels[i]].unique()\n",
    "            dict_i = dict(zip(values, range(len(values)))) # transform every unique object/string into numbers\n",
    "            matrix_data = matrix_data.replace({labels[i]:dict_i})\n",
    "    return matrix_data\n",
    "\n",
    "# previous version's method\n",
    "#weather_dict = dict([(\"sunny\",0), (\"overcast\",1), (\"rainy\",2)])\n",
    "#label_dict = dict([(\"no\",0), (\"yes\",1)])\n",
    "#newdf = df.replace({\"outlook\":weather_dict})\n",
    "#newdf = newdf.replace({\"play\":label_dict})\n",
    "\n",
    "newdf = transform_to_numeric(df)\n",
    "#print(newdf.head, \"\\n\")\n",
    "\n",
    "# scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() #feature_range=(0,1) (min_val, max_val)\n",
    "scaler.fit(newdf)\n",
    "data_matrix = scaler.transform(newdf) # di tahap ini, data sudah menjadi numpy array\n",
    "#print(data_matrix, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input number of hidden layers: 3\n",
      "Input number of nodes for hidden layer 0 : 2\n",
      "Input number of nodes for hidden layer 1 : 3\n",
      "Input number of nodes for hidden layer 2 : 4\n",
      "Input number of epochs: 1\n",
      "Input the batch size: 1\n",
      "[2. 3. 4.]\n",
      "[2. 3. 4. 1.]\n"
     ]
    }
   ],
   "source": [
    "# input and main program\n",
    "\n",
    "while True:\n",
    "    hidden_layers = int(input(\"Input number of hidden layers: \"))\n",
    "    if (hidden_layers <= 10 and hidden_layers >= 0):\n",
    "        break\n",
    "    else:\n",
    "        print(\"# of hidden layers must be a positive integer and no more than 10.\")\n",
    "\n",
    "nb_nodes = np.empty(hidden_layers)\n",
    "for i in range(hidden_layers):\n",
    "    nb_nodes[i] = int(input(\"Input number of nodes for hidden layer %d : \" % i))\n",
    "    \n",
    "momentum = 0.01 # manual in-code setting\n",
    "learning_rate = 0.01 # manual in-code setting\n",
    "\n",
    "epoch = int(input(\"Input number of epochs: \"))\n",
    "batch_size = int(input(\"Input the batch size: \"))\n",
    "\n",
    "result = mini_batch_gradient_descent(data_matrix, hidden_layers, nb_nodes, momentum, learning_rate, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#playground\n",
    "\n",
    "#print(np.random.randn(10,9) * np.sqrt(1/9))\n",
    "#x = np.empty((3, 4))\n",
    "#a = np.zeros( (3,4) )\n",
    "#b = np.zeros( (2,3) )\n",
    "#c = np.stack((x,a)) \n",
    "#np.concatenate((c, [a]))\n",
    "#testarr = np.empty((3,4,5))\n",
    "\n",
    "#emptyarr = list()\n",
    "#a = np.random.randn(3, 5) * np.sqrt(2/(5+3))\n",
    "#b = np.random.randn(5, 2) * np.sqrt(2/(5+2))\n",
    "#emptyarr.append(a)\n",
    "#emptyarr.append(b)\n",
    "#print(emptyarr)\n",
    "\n",
    "# main prog testing\n",
    "#hidden_layers = 3\n",
    "#nb_nodes = [3,4,2]\n",
    "\n",
    "#testarr = initialize_weights(3, [3,4,2], 5)\n",
    "#for i in range(len(testarr)):\n",
    "#    print(testarr[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
