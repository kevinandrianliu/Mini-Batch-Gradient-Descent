{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp\n",
    "import copy\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward helper methods\n",
    "def sigmoid(value):\n",
    "    if value < 0:\n",
    "        return 1 - 1 / (1 + exp(value))\n",
    "    else:\n",
    "        return 1.0/(1+exp(value * (-1)))\n",
    "\n",
    "def sigma(matrix_weight, matrix_input, bias=0):\n",
    "    # Prereq: len(arr_weight) = len(arr_input)\n",
    "    return matrix_weight.dot(matrix_input.transpose()) + bias\n",
    "\n",
    "# hidden_layer = int (number of hidden layers)\n",
    "# nb_nodes = arr[int] (number of nodes per hidden layer)\n",
    "# len_input_matrix = int (number of features)\n",
    "# Output: List of Matrixes\n",
    "# Method: He initialization\n",
    "# Link: https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78\n",
    "def initialize_weights(hidden_layer, nb_nodes, len_input_matrix):\n",
    "    arr_weight_this_batch = list()\n",
    "    for i in range(hidden_layer):\n",
    "        if i==0:\n",
    "            nb_nodes_prev = len_input_matrix\n",
    "        else:\n",
    "            nb_nodes_prev = nb_nodes[i-1]\n",
    "        weight_matrix = np.random.randn(nb_nodes[i], nb_nodes_prev) * np.sqrt(2/(nb_nodes_prev+nb_nodes[i]))\n",
    "        arr_weight_this_batch.append(weight_matrix)\n",
    "        \n",
    "    return arr_weight_this_batch\n",
    "\n",
    "def error(feed_forward_output, target_output): #belom dibagi batch size\n",
    "    return ((target_output-feed_forward_output)**2)\n",
    "\n",
    "def err_yes(value):\n",
    "    return (value > 0.15) # 5% fault tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation and Update Weight helper methods\n",
    "\n",
    "# hidden_layer = int (number of hidden layers)\n",
    "# nb_nodes = arr[int] (number of nodes per hidden layer)\n",
    "# len_input_matrix = int (number of features)\n",
    "# Output: List of Matrixes\n",
    "# Method: He initialization\n",
    "# Link: https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78\n",
    "def initialize_errors(hidden_layer, nb_nodes):\n",
    "    arr_neuron_errors = list()\n",
    "    for i in range(hidden_layer):\n",
    "        arr_error = np.empty(nb_nodes[i])\n",
    "        arr_neuron_errors.append(arr_error)\n",
    "    \n",
    "    return arr_neuron_errors\n",
    "\n",
    "def propagate_error_output_layer(feed_forward_output, target_output):\n",
    "    return feed_forward_output*(1-feed_forward_output)*(target_output-feed_forward_output)\n",
    "\n",
    "def propagate_error_hidden_layer_neuron(sigma_output, error_contribution):\n",
    "    return sigmoid(sigma_output) * (1 - sigmoid(sigma_output)) * error_contribution\n",
    "\n",
    "# error = neuron's error\n",
    "def update_weight_neuron(weight_prev_prev, weight_prev, learning_rate, momentum, error, input_neuron):\n",
    "    # weight_prev_prev = previous of weight_prev\n",
    "    return weight_prev + weight_prev_prev * learning_rate + momentum*error*input_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_matrix = matrix[float] (data) (asumsi, kolom terakhir adalah hasil klasifikasi)\n",
    "# hidden_layers = int (number of hidden layers)\n",
    "# nb_nodes = arr[int] (number of nodes per hidden layer)\n",
    "# nu = float (momentum)\n",
    "# alfa = float (learning rate)\n",
    "# epoch = int (number of training loops)\n",
    "# batch_size = int (mini-batch)\n",
    "# output = FFNN prediction model (list of matrix)\n",
    "def mini_batch_gradient_descent(input_matrix, hidden_layer, nb_nodes, nu, alfa, epoch, batch_size=1):\n",
    "    \n",
    "    #transpose-slicing, memisah input dan label\n",
    "    col_width = input_matrix.shape[1]\n",
    "    input_col_width = col_width - 1\n",
    "    input_data = (input_matrix.transpose()[0:input_col_width]).transpose()\n",
    "    label_data = (input_matrix.transpose()[input_col_width:col_width]).transpose()\n",
    "    #print(input_data, \"\\n\")\n",
    "    #print(label_data, \"\\n\")\n",
    "    \n",
    "    hidden_layer += 1\n",
    "    nb_nodes = np.append(nb_nodes, [1])\n",
    "    arr_neuron_errors = initialize_errors(hidden_layer, nb_nodes)\n",
    "    arr_weight_this_batch = initialize_weights(hidden_layer, nb_nodes, input_col_width)\n",
    "    \n",
    "    for no_epoch in range(epoch):\n",
    "        arr_weight_prev_batch = copy.deepcopy(arr_weight_this_batch) # tracking previous state of weights\n",
    "            \n",
    "        for no_input_data in range(len(input_data)):\n",
    "            # Feed Forward\n",
    "            all_sigma_values = list()\n",
    "            for no_hidden_layer in range(hidden_layer):\n",
    "                if no_hidden_layer == 0:\n",
    "                    all_sigma_values.append(sigma(arr_weight_this_batch[no_hidden_layer], input_data[no_input_data]))\n",
    "                else:\n",
    "                    all_sigma_values.append(sigma(arr_weight_this_batch[no_hidden_layer], all_sigma_values[no_hidden_layer-1]))\n",
    "                for no_rows in range(len(all_sigma_values[no_hidden_layer])):\n",
    "                    all_sigma_values[no_hidden_layer][no_rows] = sigmoid(all_sigma_values[no_hidden_layer][no_rows])\n",
    "            #Result of sigma will be array with 1 element only, so it's safe to select like this\n",
    "            \n",
    "            error_value = error(all_sigma_values[hidden_layer - 1][0], label_data[no_input_data])[0]\n",
    "            \n",
    "            #print(\"From data in epoch \", no_epoch+1, \" and no_input \", no_input_data+1, \" has error \", str(error_value))\n",
    "            \n",
    "            if (err_yes(error_value)):\n",
    "                # Back Propagation\n",
    "                output_error = propagate_error_output_layer(all_sigma_values[hidden_layer-1][0], label_data[no_input_data])\n",
    "                arr_neuron_errors[hidden_layer - 1][0] = output_error\n",
    "                \n",
    "                for no_hidden_layer in range(hidden_layer-2, -1, -1):\n",
    "                    for neuron in range(nb_nodes[no_hidden_layer]):\n",
    "                        # pencarian error_contribution\n",
    "                        error_contribution = 0\n",
    "                        for output_neuron in range(nb_nodes[no_hidden_layer+1]):\n",
    "                            error_contribution += arr_weight_this_batch[no_hidden_layer + 1][output_neuron][neuron] * arr_neuron_errors[no_hidden_layer + 1][output_neuron]\n",
    "                            \n",
    "                        arr_neuron_errors[no_hidden_layer][neuron] = propagate_error_hidden_layer_neuron(all_sigma_values[no_hidden_layer][neuron], error_contribution)\n",
    "                \n",
    "                # Update Weights\n",
    "                for no_hidden_layer in range(1, hidden_layer):\n",
    "                    for neuron in range(nb_nodes[no_hidden_layer]):\n",
    "                        for weight in range(len(arr_weight_this_batch[no_hidden_layer][neuron])):\n",
    "                            arr_weight_this_batch[no_hidden_layer][neuron][weight] = update_weight_neuron(arr_weight_prev_batch[no_hidden_layer][neuron][weight], arr_weight_this_batch[no_hidden_layer][neuron][weight], nu, alfa, arr_neuron_errors[no_hidden_layer][neuron], all_sigma_values[no_hidden_layer-1][weight])\n",
    "                #khusus hidden layer pertama, masukan dari input data\n",
    "                for neuron in range(nb_nodes[0]):\n",
    "                    for weight in range(input_col_width):\n",
    "                        arr_weight_this_batch[0][neuron][weight] = update_weight_neuron(arr_weight_prev_batch[0][neuron][weight], arr_weight_this_batch[0][neuron][weight], nu, alfa, arr_neuron_errors[0][neuron], input_data[no_input_data][weight])\n",
    "                        \n",
    "    return arr_weight_this_batch\n",
    "\n",
    "def predict_2classes(model, arr_features, label): #blom sempat bikin klasifier untuk lebih dari 1 kelas    \n",
    "    all_sigma_values = list()\n",
    "    for no_hidden_layer in range(len(model)):\n",
    "        if (no_hidden_layer == 0):\n",
    "            all_sigma_values.append(sigma(model[no_hidden_layer], arr_features))\n",
    "        else:\n",
    "            all_sigma_values.append(sigma(model[no_hidden_layer], all_sigma_values[no_hidden_layer-1]))\n",
    "        for no_rows in range(len(all_sigma_values[no_hidden_layer])):\n",
    "            all_sigma_values[no_hidden_layer][no_rows] = sigmoid(all_sigma_values[no_hidden_layer][no_rows])\n",
    "    \n",
    "    error_value = error(all_sigma_values[len(model) - 1][0], label)[0]\n",
    "    return (error_value < 0.5) #scaling (0, 1)\n",
    "    \n",
    "    \n",
    "def accuracy(model, input_matrix):\n",
    "    #transpose-slicing, memisah input dan label\n",
    "    col_width = input_matrix.shape[1]\n",
    "    input_col_width = col_width - 1\n",
    "    input_data = (input_matrix.transpose()[0:input_col_width]).transpose()\n",
    "    label_data = (input_matrix.transpose()[input_col_width:col_width]).transpose()\n",
    "    #print(input_data, \"\\n\")\n",
    "    #print(label_data, \"\\n\")\n",
    "    \n",
    "    true_count = 0\n",
    "    false_count = 0\n",
    "    for no_input_data in range(len(input_data)):\n",
    "        if (predict_2classes(model, input_data[no_input_data], label_data[no_input_data])):\n",
    "            true_count += 1\n",
    "        else:\n",
    "            false_count += 1\n",
    "    return true_count / (true_count + false_count) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input .csv filename: weather.csv\n",
      "File loaded successfuly.\n"
     ]
    }
   ],
   "source": [
    "# dataset load\n",
    "#df = pd.read_csv(\"weather.csv\")\n",
    "csv_string = input(\"Input .csv filename: \")\n",
    "try:\n",
    "    df = pd.read_csv(csv_string)\n",
    "except:\n",
    "    print(\"File not found.\")\n",
    "    ###quit() (di file Python, pake ini)\n",
    "print(\"File loaded successfuly.\")\n",
    "#print(df.head, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " # dataset preprocess\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    # transform non-numeric data to numeric data\n",
    "    types = df.dtypes\n",
    "    labels = df.columns.values # because pandas select columns using column names\n",
    "    def transform_to_numeric(matrix_data):\n",
    "        for i in range(matrix_data.shape[1]):\n",
    "            type_i = types[i]\n",
    "            if (type_i == object):\n",
    "                values = matrix_data[labels[i]].unique()\n",
    "                dict_i = dict(zip(values, range(len(values)))) # transform every unique object/string into numbers\n",
    "                matrix_data = matrix_data.replace({labels[i]:dict_i})\n",
    "            elif (type_i == bool):\n",
    "                matrix_data[labels[i]] = matrix_data[labels[i]].astype(int)\n",
    "        return matrix_data\n",
    "\n",
    "    newdf = transform_to_numeric(df)\n",
    "    #print(newdf.head, \"\\n\")\n",
    "\n",
    "        # scaling\n",
    "    def scale_data(matrix_data, min_val, max_val):\n",
    "\n",
    "        def scaling(value):\n",
    "            return (value - minValue)*(max_val - min_val)/(maxValue - minValue) + min_val\n",
    "\n",
    "        for x in range(matrix_data.shape[1]):\n",
    "            minValue = matrix_data[labels[x]].min()\n",
    "            maxValue = matrix_data[labels[x]].max()\n",
    "            matrix_data[labels[x]] = matrix_data[labels[x]].apply(scaling)\n",
    "        return matrix_data\n",
    "\n",
    "    data_matrix = scale_data(newdf, 0, 1)\n",
    "    #print(data_matrix, \"\\n\")\n",
    "    data_matrix = data_matrix.to_numpy() #convert pandas dataframe to numpy array\n",
    "    #print(data_matrix, \"\\n\")\n",
    "\n",
    "    return data_matrix\n",
    "\n",
    "def split_train_test(matrix_data, test_portion):\n",
    "    total_data = len(matrix_data)\n",
    "    total_data_for_test = int(round(test_portion * total_data, 0))\n",
    "    total_data_for_train = total_data - total_data_for_test\n",
    "    return(matrix_data[0:total_data_for_train], matrix_data[total_data_for_train:total_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input number of hidden layers: 2\n",
      "Input number of nodes for hidden layer 0 : 1\n",
      "Input number of nodes for hidden layer 1 : 1\n",
      "Input momentum: 0.2\n",
      "Input learning rate: 0.2\n",
      "Input epoch: 5\n",
      "Input the batch size: 1\n",
      "Input the test size: 0.25\n",
      "Accuracy:  75.0\n"
     ]
    }
   ],
   "source": [
    "# input and main program\n",
    "\n",
    "while True:\n",
    "    hidden_layers = int(input(\"Input number of hidden layers: \"))\n",
    "    if (hidden_layers <= 10 and hidden_layers >= 0):\n",
    "        break\n",
    "    else:\n",
    "        print(\"# of hidden layers must be a positive integer and no more than 10.\")\n",
    "\n",
    "nb_nodes = np.empty(hidden_layers)\n",
    "for i in range(hidden_layers):\n",
    "    while True:\n",
    "        nb_nodes[i] = int(input(\"Input number of nodes for hidden layer %d : \" % i))\n",
    "        if (nb_nodes[i] > 0):\n",
    "            break\n",
    "        else:\n",
    "            print(\"# of nodes must be a positive integer.\")\n",
    "    \n",
    "    \n",
    "while True:\n",
    "    momentum = float(input(\"Input momentum: \"))\n",
    "    if (momentum <= 1 and momentum >= 0):\n",
    "        break\n",
    "    else:\n",
    "        print(\"Momentum must be between 0 and 1.\")\n",
    "\n",
    "while True:\n",
    "    learning_rate = float(input(\"Input learning rate: \"))\n",
    "    if (learning_rate <= 1 and learning_rate >= 0):\n",
    "        break\n",
    "    else:\n",
    "        print(\"Learning rate must be between 0 and 1.\")\n",
    "\n",
    "while True:\n",
    "    epoch = int(input(\"Input epoch: \"))\n",
    "    if (epoch > 0):\n",
    "        break\n",
    "    else:\n",
    "        print(\"Epoch must be a positive integer.\")\n",
    "\n",
    "while True:\n",
    "    batch_size = int(input(\"Input the batch size: \"))\n",
    "    if (batch_size > 0):\n",
    "        break\n",
    "    else:\n",
    "        print(\"Batch size must be a positive integer.\")\n",
    "\n",
    "while True:\n",
    "    test_size = float(input(\"Input the test size: \"))\n",
    "    if (test_size > 0 and test_size < 1):\n",
    "        break\n",
    "    else:\n",
    "        print(\"Test size must be between 0 and 1.\")\n",
    "\n",
    "data_matrix = preprocess_dataframe(df)\n",
    "train_matrix, test_matrix = split_train_test(data_matrix, test_size)\n",
    "#print(train_matrix)\n",
    "#print(test_matrix)\n",
    "\n",
    "nb_nodes = nb_nodes.astype(int) #diperlukan karena dianggap float dalam fungsi randn jika tak diubah\n",
    "custom_model = mini_batch_gradient_descent(train_matrix, hidden_layers, nb_nodes, momentum, learning_rate, epoch, batch_size)\n",
    "print(\"Accuracy: \", accuracy(custom_model, test_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
